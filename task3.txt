"""
Neural Style Transfer â€” CODTECH

A self-contained Python script / notebook-ready file implementing neural style
transfer using PyTorch. Use this in a Jupyter notebook or run as a script.

Deliverable: examples of styled images saved to disk and displayed inline.

Dependencies:
- torch
- torchvision
- Pillow
- matplotlib
- tqdm (optional)

Run in a notebook or from command line. Example usage is at the bottom of this
file.

By: ChatGPT (for your CODTECH internship)
"""

import os
import copy
from PIL import Image
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
import torchvision.models as models

# ---------------------- Config / Hyperparams ----------------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Size of the output image (smaller -> faster)
imsize = 512 if torch.cuda.is_available() else 256

# Weights for the different losses
content_weight = 1e4
style_weight = 1e2

# Number of optimization steps
num_steps = 300

# ---------------------- Utilities ----------------------

loader = transforms.Compose([
    transforms.Resize((imsize, imsize)),
    transforms.ToTensor()
])

unloader = transforms.ToPILImage()


def image_loader(image_path):
    """Load image and convert to torch tensor with batch dimension."""
    image = Image.open(image_path).convert('RGB')
    image = loader(image).unsqueeze(0)
    return image.to(device, torch.float)


def show_image(tensor, title=None):
    image = tensor.cpu().clone()
    image = image.squeeze(0)
    image = unloader(image)
    plt.figure(figsize=(6,6))
    plt.axis('off')
    if title:
        plt.title(title)
    plt.imshow(image)
    plt.show()


# ---------------------- Model pieces ----------------------

class Normalization(nn.Module):
    """Normalizes input using ImageNet mean/std used for pretrained VGG."""
    def __init__(self, mean, std):
        super(Normalization, self).__init__()
        # .view the mean and std so they can directly work with image Tensor of shape [B x C x H x W]
        self.mean = torch.tensor(mean).view(-1, 1, 1).to(device)
        self.std = torch.tensor(std).view(-1, 1, 1).to(device)

    def forward(self, img):
        return (img - self.mean) / self.std


class ContentLoss(nn.Module):
    def __init__(self, target, weight=1):
        super(ContentLoss, self).__init__()
        # detach target from graph, we don't need grads for it
        self.target = target.detach()
        self.weight = weight
        self.loss = 0

    def forward(self, input):
        self.loss = nn.functional.mse_loss(input, self.target) * self.weight
        # return input so the module can be inserted like a layer
        return input


def gram_matrix(input):
    batch, ch, h, w = input.size()
    features = input.view(batch * ch, h * w)
    G = torch.mm(features, features.t())
    # normalize by number of elements
    return G.div(batch * ch * h * w)


class StyleLoss(nn.Module):
    def __init__(self, target_feature, weight=1):
        super(StyleLoss, self).__init__()
        self.weight = weight
        self.target = gram_matrix(target_feature).detach()
        self.loss = 0

    def forward(self, input):
        G = gram_matrix(input)
        self.loss = nn.functional.mse_loss(G, self.target) * self.weight
        return input


# ---------------------- Build the model ----------------------

def get_style_model_and_losses(cnn, normalization_mean, normalization_std,
                                style_img, content_img,
                                content_layers=['conv_4'],
                                style_layers=['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']):
    # copy the cnn to avoid changing it
    cnn = copy.deepcopy(cnn)

    normalization = Normalization(normalization_mean, normalization_std).to(device)

    content_losses = []
    style_losses = []

    # sequential model we'll build
    model = nn.Sequential(normalization)

    i = 0  # conv layer counter
    for layer in cnn.features.children():
        if isinstance(layer, nn.Conv2d):
            i += 1
            name = 'conv_{}'.format(i)
        elif isinstance(layer, nn.ReLU):
            name = 'relu_{}'.format(i)
            # In-place ReLU from torchvision will modify activations; use out-of-place
            layer = nn.ReLU(inplace=False)
        elif isinstance(layer, nn.MaxPool2d):
            name = 'pool_{}'.format(i)
        elif isinstance(layer, nn.BatchNorm2d):
            name = 'bn_{}'.format(i)
        else:
            name = str(layer)

        model.add_module(name, layer)

        if name in content_layers:
            target = model(content_img).detach()
            content_loss = ContentLoss(target, weight=content_weight)
            model.add_module("content_loss_{}".format(i), content_loss)
            content_losses.append(content_loss)

        if name in style_layers:
            target_feature = model(style_img).detach()
            style_loss = StyleLoss(target_feature, weight=style_weight)
            model.add_module("style_loss_{}".format(i), style_loss)
            style_losses.append(style_loss)

    # now we trim off the layers after the last content and style losses
    for i in range(len(model) - 1, -1, -1):
        if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):
            break
    model = model[:(i + 1)]

    return model, style_losses, content_losses


# ---------------------- Style transfer routine ----------------------

def run_style_transfer(cnn, normalization_mean, normalization_std,
                       content_img, style_img, input_img, num_steps=num_steps,
                       style_weight=style_weight, content_weight=content_weight):
    print('Building the style transfer model...')
    model, style_losses, content_losses = get_style_model_and_losses(cnn,
                                                                     normalization_mean, normalization_std,
                                                                     style_img, content_img)
    optimizer = optim.LBFGS([input_img.requires_grad_()])

    print('Optimizing...')
    run = [0]
    while run[0] <= num_steps:

        def closure():
            input_img.data.clamp_(0, 1)
            optimizer.zero_grad()
            model(input_img)
            style_score = 0
            content_score = 0

            for sl in style_losses:
                style_score += sl.loss
            for cl in content_losses:
                content_score += cl.loss

            loss = style_score + content_score
            loss.backward()

            if run[0] % 50 == 0:
                print("Step {}: Style Loss: {:4f} Content Loss: {:4f}".format(
                    run[0], style_score.item(), content_score.item()))

            run[0] += 1
            return loss

        optimizer.step(closure)

    # a last clamp
    input_img.data.clamp_(0, 1)
    return input_img


# ---------------------- Example / CLI ----------------------

if __name__ == '__main__':
    import argparse

    parser = argparse.ArgumentParser(description='Neural Style Transfer (PyTorch)')
    parser.add_argument('--content', type=str, default=None, help='Path to content image')
    parser.add_argument('--style', type=str, default=None, help='Path to style image')
    parser.add_argument('--output', type=str, default='output.jpg', help='Output image path')
    parser.add_argument('--steps', type=int, default=num_steps, help='Optimization steps')
    parser.add_argument('--size', type=int, default=imsize, help='Output image size')
    args = parser.parse_args()

    # If running as a notebook, you can set these variables directly instead of using CLI
    if args.content is None or args.style is None:
        print('\nNo content/style provided via CLI. If running this file in a notebook, set the variables manually.\n')
        # Example inline placeholders (uncomment and modify to run in notebook)
        # content_img_path = 'path/to/your/content.jpg'
        # style_img_path = 'path/to/your/style.jpg'
        # output_path = 'styled_output.jpg'
        # Use the following block in a notebook environment:
        # content_img = image_loader(content_img_path)
        # style_img = image_loader(style_img_path)
        # input_img = content_img.clone()
        # cnn = models.vgg19(pretrained=True).to(device).eval()
        # output = run_style_transfer(cnn, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225],
        #                            content_img, style_img, input_img, num_steps=300)
        # show_image(output, title='Output Image')
        # output_pil = output.cpu().clone().squeeze(0)
        # unloader(output_pil).save(output_path)
        
        # For convenience, we'll stop here when no CLI args supplied.
        exit(0)

    # otherwise proceed with provided paths
    content_path = args.content
    style_path = args.style
    out_path = args.output
    imsize = args.size

    # reload transforms with correct size
    loader = transforms.Compose([
        transforms.Resize((imsize, imsize)),
        transforms.ToTensor()
    ])

    # Load images
    content_img = image_loader(content_path)
    style_img = image_loader(style_path)

    assert content_img.size() == style_img.size(), "Content and style images must be the same size"

    # Input image: we start from the content image (you can also start from random noise)
    input_img = content_img.clone()

    # Load pretrained VGG19
    cnn = models.vgg19(pretrained=True).to(device).eval()

    # Run style transfer
    output = run_style_transfer(cnn, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225],
                                content_img, style_img, input_img, num_steps=args.steps)

    # Save and display
    saved = unloader(output.cpu().squeeze(0))
    saved.save(out_path)
    print(f"Saved styled image to {out_path}")

    # If running in an environment that can show images, also display
    try:
        show_image(output, title='Final Output')
    except Exception:
        pass


# ---------------------- Notes & Tips ----------------------
# - This implementation follows the original Gatys et al. optimization approach.
# - For faster results, try using the faster Johnson et al. feed-forward network
#   approach (train a generator network for a single style), which is much faster
#   at inference time but requires training for each style.
# - Play with content_weight and style_weight to control how "stylized" the
#   output becomes. Increase style_weight for stronger style.
# - If you want to process high-resolution images, consider using multi-scale
#   approaches: run at a smaller size, then upsample and refine.
# - To run in Colab, upload images to the session storage and pass the paths to
#   the script, or modify the notebook cells to load images from URLs.
